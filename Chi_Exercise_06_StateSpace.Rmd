---
title: "Activity 6 - State-space models"
author: "Student: Chi Chen; Professor: Micheal Dietze"
date: "March 11, 2016"
output: html_document
---
========================================================

## Make a function for the JAGS model and shaded CI plot
```{r}
predict.JAGS <- function(time,y) {
library(rjags)
RandomWalk = "
model{

#### Data Model
for(i in 1:n){
y[i] ~ dnorm(x[i],tau_obs)
}

#### Process Model
for(i in 2:n){
x[i]~dnorm(x[i-1],tau_add)
}

#### Priors
x[1] ~ dnorm(x_ic,tau_ic)
tau_obs ~ dgamma(a_obs,r_obs)
tau_add ~ dgamma(a_add,r_add)
}
"

data <- list(y=log(y),n=length(y),x_ic=log(1000),tau_ic=100,a_obs=1,r_obs=1,a_add=1,r_add=1)

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(log(y.samp))),tau_obs=5/var(log(y.samp)))
}

j.model   <- jags.model (file = textConnection(RandomWalk),
                        data = data,
                        inits = init,
                        n.chains = 3)

## burn-in
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("tau_add","tau_obs"),
                            n.iter = 1000)
# Only to plot 1000 iterations. It takes so much time to plot 10000 iterations.  
plot(jags.out) 

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("x","tau_add","tau_obs"),
                            n.iter = 10000)
 
}

# function for shaded confidence interval
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}

```

## Original Model (weekly frequency)

```{r}
gflu = read.csv("http://www.google.org/flutrends/about/data/flu/us/data.txt",skip=11)
time = as.Date(gflu$Date)
y = gflu$Massachusetts

# plot original weekly observation data
plot(time,y,type='l',ylab="Flu Index",lwd=2,log='y',main='original weekly observations')

jags.out.original = predict.JAGS(time,y)

par(mfrow=c(1,1))

# plot the original result (weekly observation frequency)
time.rng = c(1,length(time)) ## adjust to zoom in and out
out <- as.matrix(jags.out.original)
ci <- apply(exp(out[,3:ncol(out)]),2,quantile,c(0.025,0.5,0.975))

plot(time,ci[2,],type='n',ylim=range(y,na.rm=TRUE),ylab="Flu Index",log='y',xlim=time[time.rng], main='Model fitted by full weekly observations')
## adjust x-axis label to be monthly if zoomed
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='month'), format = "%Y-%m")
}
ciEnvelope(time,ci[1,],ci[3,],col="lightBlue")
points(time,y,pch="+",cex=0.5)

layout(matrix(c(1,2,3,3),2,2,byrow=TRUE))
hist(1/sqrt(out[,1]),main=colnames(out)[1])
hist(1/sqrt(out[,2]),main=colnames(out)[2])
plot(out[,1],out[,2],pch=".",xlab=colnames(out)[1],ylab=colnames(out)[2])
cor(out[,1:2])

#Save this for later use...
ci.origin = ci
```



Assignment:
-----------

To look at how observation frequency affects data assimilation, convert 3 out of every 4 observations to NA (i.e. treat the data as approximately monthly) and refit the model. 

* Generate a time-series plot for the CI of x that includes the observations (as above). Use a different color and symbol to differentiate observations that were included in the model versus those that were converted to NA's.
* Compare the CI between the two runs.
* Generate a predicted (median) vs observed plot for the data points that were removed
* Comment on the accuracy and precision of the estimates.

## Answers
1. The invertal of CI is much larger in the 2nd run.
2. Generate a predicted (median) vs observed plot for the data points that were removed: Generally, they still have a linear relationship. The linear relationship is stronger when flu index is small(i.e.less than 1800), and it becomes weaker when flu index get larger. Also, their slope is way off 1. Predicted median is smaller than observed data.
3. Comment on the accuracy and precision: From the histogram of tau_obs and tau_add, those in the 2nd run are much larger than the 1st run. I also tried to use the median to represent the accuracy of the two runs. The scatter plots (observed vs predicted median) show that the 1st run is more accurate. In addition, I did OLS to check how much variantion in observed data (R square) can be explianed by the predicted median of the two runs. The results also show that the 1st run is more accurate. In addtion, the two OLS models show that the slope of the 1st run is closer to 1.  About the precision, the confidence interval can tell the whole thing. Obviously, the 1st run is better.
4. SO, my conclusion is: increasing the data frequency can improve our prediction model.

## Convert 3 out of every 4 observatoins to NA
```{r}
time = as.Date(gflu$Date)
y = gflu$Massachusetts
# convert 3 out of every 4 observations to NA
index = seq(1,length(y),4)
y[-index] = NA

y.removed = gflu$Massachusetts
y.removed[index] = NA

jags.out.removed = predict.JAGS(time,y)

time.rng = c(1,length(time)) ## adjust to zoom in and out
out <- as.matrix(jags.out.removed)
ci <- apply(exp(out[,3:ncol(out)]),2,quantile,c(0.025,0.5,0.975))

# Generate a time-series plot for the CI of x that includes the observations.
plot(time,ci[2,],type='n',ylim=range(y,na.rm=TRUE),ylab="Flu Index",log='y',xlim=time[time.rng],main='Model fitted by reduced frequency observations')

## adjust x-axis label to be monthly if zoomed
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='month'), format = "%Y-%m")
}
ciEnvelope(time,ci[1,],ci[3,],col="lightBlue")
points(time,y,pch="+",cex=0.5)
points(time,y.removed,pch='*',col='red',cex=0.5)
legend("topleft",lty=c(1,0,0),c('95% CI','data used to fit model','data not used to fit model'),col=c('lightBlue','black','red'),pch=c(0,'+','*'),lwd=2,cex=0.65)


#Generate a predicted (median) vs observed plot for the data points that were removed
predicted.median = ci[2,]
predicted.median[index]=NA
plot(predicted.median,y.removed,main='Predicted median VS observed')

#About the accuracy and precision of the two runs: scatter plot of observed vs predicted median
plot(ci.origin[2,],gflu$Massachusetts,main='The 1st run')
plot(ci[2,],gflu$Massachusetts,main='The 2nd run')

# check how much variantion in observed data could be explained by the predicted medians from the two runs
fit1 =  lm(gflu$Massachusett~ ci.origin[2,]) # full data, weekly observations
fit2 =  lm(gflu$Massachusett~ ci[2,]) #removed data, monthly data
summary(fit1)
summary(fit2)



layout(matrix(c(1,2,3,3),2,2,byrow=TRUE))
hist(1/sqrt(out[,1]),main=colnames(out)[1])
hist(1/sqrt(out[,2]),main=colnames(out)[2])
plot(out[,1],out[,2],pch=".",xlab=colnames(out)[1],ylab=colnames(out)[2])
cor(out[,1:2])

```

Extra Credit:
-------------

Return to the original data and instead of removing 3/4 of the data remove the last 40 observations (convert to NA) and refit the model to make a forecast for this period

* Generate a time-series plot for the CI of x that includes the observations (as above but zoom the plot on the last ~80 observations). Use a different color and symbol to differentiate observations that were included in the model versus those that were converted to NA's.
* Comment on how well the random walk model performed (both accuracy and precision) and how it might be modified to improve both these criteria.

## Answer Extra Credit
1. From the histogram of tau_obs and tau_add, they look very similar to the 1st run (the full weekly frequency run). In particular, in the time-series plot, the accuracy and precision are similiar in the first 1:580 points, but they exploded in the last 40 predictions.
2. If we want to improve both accuracy and precision, we need to have more observations in those "future period". In this case, the accuracy and precision of random walk model are largely depended on the current data. It also implies that the future preditions are largely depended on the current data (if we don't have the future data).

```{r}
time = as.Date(gflu$Date)
y = gflu$Massachusetts
# convert last 40 observations to NA
index = 1:(length(y)-40)
y[-index] = NA

y.removed = gflu$Massachusetts
y.removed[index] = NA

jags.out.removed = predict.JAGS(time,y)

time.rng = c(length(time)-79,length(time)) ## adjust to zoom in and out
out <- as.matrix(jags.out.removed)
ci <- apply(exp(out[,3:ncol(out)]),2,quantile,c(0.025,0.5,0.975))

# Generate a time-series plot for the CI of x that includes the observations.
plot(time,ci[2,],type='n',ylim=range(y,na.rm=TRUE),ylab="Flu Index",log='y',xlim=time[time.rng],main='model fitted by removing last 40 observations')

## adjust x-axis label to be monthly if zoomed
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='month'), format = "%Y-%m")
}
ciEnvelope(time,ci[1,],ci[3,],col="lightBlue")
points(time,y,pch="+",cex=0.5)
points(time,y.removed,pch='*',col='red',cex=0.5)
legend("topleft",lty=c(1,0,0),c('95% CI','data used to fit model','data not used to fit model'),col=c('lightBlue','black','red'),pch=c(0,'+','*'),lwd=2,cex=0.65)

layout(matrix(c(1,2,3,3),2,2,byrow=TRUE))
hist(1/sqrt(out[,1]),main=colnames(out)[1])
hist(1/sqrt(out[,2]),main=colnames(out)[2])
plot(out[,1],out[,2],pch=".",xlab=colnames(out)[1],ylab=colnames(out)[2])
cor(out[,1:2])
```